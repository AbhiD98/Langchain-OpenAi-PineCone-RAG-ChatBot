{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaac053c",
   "metadata": {},
   "source": [
    "# RAG Chat-Bot Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a15f8d",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## This script sets up a Streamlit-based chatbot application using PDF documents as the knowledge base. It involves several key steps:\n",
    "\n",
    "## Import Libraries: \n",
    "Necessary libraries are imported for document loading, text splitting, environment variable access, Pinecone vector database, embedding, chat model, retrieval-based QA, and Streamlit web app.\n",
    "\n",
    "## Load Environment Variables: \n",
    "API keys for Pinecone and OpenAI are loaded from a .env file.\n",
    "\n",
    "## Document Pre-Processing:\n",
    "PDF documents from the \"document/\" directory are loaded and split into chunks of 1000 characters.\n",
    "\n",
    "## Embedding Database: \n",
    "A Pinecone index is created if it doesn't exist, and the split documents are embedded using OpenAI's embeddings. This is cached using Streamlit's st.cache_resource for efficiency.\n",
    "\n",
    "## Initialize Models: \n",
    "The ChatOpenAI model is instantiated and the embedding database is created.\n",
    "\n",
    "## Retrieve Answers: \n",
    "A function is defined to retrieve answers to user queries using a retrieval-based question-answering chain.\n",
    "\n",
    "## Streamlit App: \n",
    "The main function sets up the Streamlit web app, which manages chat history, accepts user input, and displays responses. The app title is set to \"Abhi-Alemeno RAG CHAT-BOT\".\n",
    "\n",
    "\n",
    "## Use VS Code to eun main file \"main.py\" which is in same directory to launch streamlit app, in jupyter notebook it won't run efficiently...Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b5ad5",
   "metadata": {},
   "source": [
    "# Important Note\n",
    "I sincerely apologize for deviating from the initial plan of using local embedding models and LLM (Language Model). Unfortunately, my laptop's low specifications made it exceedingly challenging to efficiently run both the local embedding models and the LLM simultaneously. This limitation hindered my ability to implement the project as originally intended.\n",
    "\n",
    "Given a more suitable environment or the oppertunity, I am confident that I can successfully complete the task according to the specified requirements. I appreciate your understanding and the opportunity to address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c1ddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHIMANYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d61f0f",
   "metadata": {},
   "source": [
    "Imports necessary libraries for document loading, text splitting, environment variable access, Pinecone database, embedding, chat model, retrieval-based QA, and Streamlit web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c09f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5c87e",
   "metadata": {},
   "source": [
    "Loads environment variables from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f93ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv('PINECONE_ENV')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a11fa",
   "metadata": {},
   "source": [
    "This snippet retrieves the Pinecone and OpenAI API keys, along with the Pinecone environment variable, from the environment and sets the OpenAI API key in the environment variables. This ensures secure access to the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d496affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Pre-Processing\n",
    "def doc_processing():\n",
    "    loader = PyPDFDirectoryLoader(\"document/\", glob='**/*.pdf')\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "    docs_split = text_splitter.split_documents(docs)\n",
    "    return docs_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c3661",
   "metadata": {},
   "source": [
    "This function processes PDF documents:\n",
    "\n",
    "Loads all PDF files from the specified directory.\n",
    "Splits the loaded documents into smaller chunks of 1000 characters without overlap.\n",
    "Returns the split documents for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a06cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding DB\n",
    "@st.cache_resource\n",
    "def embedding_db():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    if 'abhi-alemeno' not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name='abhi-alemeno',\n",
    "            dimension=1536,  # Use 1536 dimensions for text-embedding-ada-002\n",
    "            metric='cosine',  # Use cosine similarity\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region=PINECONE_ENV\n",
    "            )\n",
    "        )\n",
    "\n",
    "    doc_split = doc_processing()\n",
    "    vectorstore = PineconeVectorStore.from_documents(doc_split, embedding=embeddings, index_name='abhi-alemeno')\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e3a45",
   "metadata": {},
   "source": [
    "This cached function sets up the embedding database:\n",
    "    \n",
    "Initializes OpenAI embeddings.\n",
    "Connects to Pinecone using the API key.\n",
    "Creates a Pinecone index if it doesn't already exist, specifying the dimension and similarity metric.\n",
    "Processes the documents and creates a Pinecone vector store from the split documents.\n",
    "Returns the vector store for use in retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "284ab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()  # Instantiate the ChatOpenAI model\n",
    "doc_db = embedding_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cc3a5",
   "metadata": {},
   "source": [
    "This snippet initializes the language model (ChatOpenAI) and creates the embedding database by calling the embedding_db function, setting up the environment for retrieval-based question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b850cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_answer(query):\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type='stuff',\n",
    "        retriever=doc_db.as_retriever(),\n",
    "    )\n",
    "    result = qa.run(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c37c0c",
   "metadata": {},
   "source": [
    "This function retrieves answers to user queries:\n",
    "\n",
    "Uses the RetrievalQA chain with the language model and document retriever.\n",
    "Runs the query through the QA chain.\n",
    "Returns the result, which is the answer to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39894ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Abhi-Alemeno RAG CHAT-BOT\")\n",
    "    \n",
    "    # Initialize chat history in session state if not present\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state[\"messages\"] = []\n",
    "    \n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "    \n",
    "    # Accept user input using chat input widget\n",
    "    prompt = st.chat_input(\"Ask Your Query...\")\n",
    "    \n",
    "    if prompt:\n",
    "        # Add user message to chat history\n",
    "        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Display user message in chat\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "        \n",
    "        # Retrieve and display assistant response\n",
    "        answer = retrieval_answer(prompt)\n",
    "        st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(answer)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a05aba",
   "metadata": {},
   "source": [
    "This function sets up and runs the Streamlit web application:\n",
    "    \n",
    "Sets the title of the web app.\n",
    "Initializes chat history in the session state.\n",
    "Displays chat messages from history on app rerun.\n",
    "Accepts user input via a chat input widget.\n",
    "Adds user messages to chat history and displays them.\n",
    "Retrieves answers to user queries and displays them.\n",
    "Runs the main function if the script is executed directly, starting the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf59c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
